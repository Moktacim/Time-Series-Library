{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Moktacim/Time-Series-Library/blob/main/PyPOTS_Tutorials.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DonG1Q5K8lFi"
      },
      "source": [
        "# 😎 Quick-start Tutorials for PyPOTS are Here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDZqwFN4899V"
      },
      "source": [
        "## Dependency Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xe-rPZzC9CoW"
      },
      "outputs": [],
      "source": [
        "# install pypots >=0.4\n",
        "! pip install pypots>=0.4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StrWh_It88lF"
      },
      "source": [
        "## 📀 Preparing the **PhysioNet-2012** dataset for this tutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yASJSepF17za",
        "outputId": "2bef2a6b-8942-4323-8ede-089fb4ca8c6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-19 09:40:46 [INFO]: Have set the random seed as 2204 for numpy and pytorch.\n",
            "2024-03-19 09:40:46 [INFO]: Loading the dataset physionet_2012 with TSDB (https://github.com/WenjieDu/Time_Series_Data_Beans)...\n",
            "2024-03-19 09:40:46 [INFO]: Starting preprocessing physionet_2012...\n",
            "2024-03-19 09:40:46 [INFO]: You're using dataset physionet_2012, please cite it properly in your work. You can find its reference information at the below link: \n",
            "https://github.com/WenjieDu/TSDB/tree/main/dataset_profiles/physionet_2012\n",
            "2024-03-19 09:40:46 [INFO]: Dataset physionet_2012 has already been downloaded. Processing directly...\n",
            "2024-03-19 09:40:46 [INFO]: Dataset physionet_2012 has already been cached. Loading from cache directly...\n",
            "2024-03-19 09:40:46 [INFO]: Loaded successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['n_classes', 'n_steps', 'n_features', 'train_X', 'train_y', 'train_ICUType', 'val_X', 'val_y', 'val_ICUType', 'test_X', 'test_y', 'test_ICUType', 'scaler', 'val_X_ori', 'test_X_ori', 'test_X_indicating_mask'])\n"
          ]
        }
      ],
      "source": [
        "from pypots.data.generating import gene_physionet2012\n",
        "from pypots.utils.random import set_random_seed\n",
        "\n",
        "set_random_seed()\n",
        "\n",
        "# Load the PhysioNet-2012 dataset\n",
        "physionet2012_dataset = gene_physionet2012(artificially_missing_rate=0.1)\n",
        "\n",
        "# Take a look at the generated PhysioNet-2012 dataset, you'll find that everything has been prepared for you,\n",
        "# data splitting, normalization, additional artificially-missing values for evaluation, etc.\n",
        "print(physionet2012_dataset.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc3SoTqg9r2M"
      },
      "source": [
        "## 🌟 Imputation Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z11KSsFu-A9q"
      },
      "outputs": [],
      "source": [
        "# Assemble the datasets for training, validating, and testing.\n",
        "\n",
        "dataset_for_training = {\n",
        "    \"X\": physionet2012_dataset['train_X'],\n",
        "}\n",
        "\n",
        "dataset_for_validating = {\n",
        "    \"X\": physionet2012_dataset['val_X'],\n",
        "    \"X_ori\": physionet2012_dataset['val_X_ori'],\n",
        "}\n",
        "\n",
        "dataset_for_testing = {\n",
        "    \"X\": physionet2012_dataset['test_X'],\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVqyqt_S9z3E"
      },
      "source": [
        "### 🚀 An example of **SAITS** for imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcxUvLNE9rNL",
        "outputId": "fe477ced-946a-4d16-e3b8-e43c3d3db621"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-18 06:21:05 [INFO]: No given device, using default device: cuda\n",
            "2024-03-18 06:21:05 [INFO]: Model files will be saved to tutorial_results/imputation/saits/20240318_T062105\n",
            "2024-03-18 06:21:05 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/saits/20240318_T062105/tensorboard\n",
            "2024-03-18 06:21:05 [INFO]: SAITS initialized with the given hyperparameters, the number of trainable parameters: 1,378,358\n",
            "2024-03-18 06:21:20 [INFO]: Epoch 001 - training loss: 0.7327, validating loss: 0.4593\n",
            "2024-03-18 06:21:26 [INFO]: Epoch 002 - training loss: 0.5173, validating loss: 0.4261\n",
            "2024-03-18 06:21:32 [INFO]: Epoch 003 - training loss: 0.4625, validating loss: 0.4070\n",
            "2024-03-18 06:21:39 [INFO]: Epoch 004 - training loss: 0.4211, validating loss: 0.3837\n",
            "2024-03-18 06:21:47 [INFO]: Epoch 005 - training loss: 0.3924, validating loss: 0.3833\n",
            "2024-03-18 06:21:53 [INFO]: Epoch 006 - training loss: 0.3744, validating loss: 0.3663\n",
            "2024-03-18 06:21:59 [INFO]: Epoch 007 - training loss: 0.3609, validating loss: 0.3615\n",
            "2024-03-18 06:22:04 [INFO]: Epoch 008 - training loss: 0.3507, validating loss: 0.3542\n",
            "2024-03-18 06:22:11 [INFO]: Epoch 009 - training loss: 0.3438, validating loss: 0.3604\n",
            "2024-03-18 06:22:16 [INFO]: Epoch 010 - training loss: 0.3371, validating loss: 0.3456\n",
            "2024-03-18 06:22:16 [INFO]: Finished training.\n",
            "2024-03-18 06:22:16 [INFO]: Saved the model to tutorial_results/imputation/saits/20240318_T062105/SAITS.pypots\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing mean absolute error: 0.2289\n"
          ]
        }
      ],
      "source": [
        "from pypots.utils.metrics import calc_mae\n",
        "from pypots.optim import Adam\n",
        "from pypots.imputation import SAITS\n",
        "\n",
        "a\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "saits.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
        "\n",
        "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
        "saits_results = saits.predict(dataset_for_testing)\n",
        "saits_imputation = saits_results[\"imputation\"]\n",
        "\n",
        "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
        "testing_mae = calc_mae(\n",
        "    saits_imputation,\n",
        "    physionet2012_dataset['test_X_ori'],\n",
        "    physionet2012_dataset['test_X_indicating_mask'],\n",
        ")\n",
        "print(f\"Testing mean absolute error: {testing_mae:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15BwxZCW-OLm"
      },
      "source": [
        "### 🚀 An example of **Transformer** for imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRUFFj11-NWj",
        "outputId": "e0e7e86c-e56d-4844-a5fc-f1a91d1c5946"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-18 06:22:17 [INFO]: No given device, using default device: cuda\n",
            "2024-03-18 06:22:17 [INFO]: Model files will be saved to tutorial_results/imputation/transformer/20240318_T062217\n",
            "2024-03-18 06:22:17 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/transformer/20240318_T062217/tensorboard\n",
            "2024-03-18 06:22:17 [INFO]: Transformer initialized with the given hyperparameters, the number of trainable parameters: 7,938,597\n",
            "2024-03-18 06:22:29 [INFO]: Epoch 001 - training loss: 1.4288, validating loss: 1.1047\n",
            "2024-03-18 06:22:39 [INFO]: Epoch 002 - training loss: 1.3915, validating loss: 1.0857\n",
            "2024-03-18 06:22:48 [INFO]: Epoch 003 - training loss: 1.3786, validating loss: 1.0966\n",
            "2024-03-18 06:22:57 [INFO]: Epoch 004 - training loss: 1.3699, validating loss: 1.0834\n",
            "2024-03-18 06:23:08 [INFO]: Epoch 005 - training loss: 1.3677, validating loss: 1.0941\n",
            "2024-03-18 06:23:18 [INFO]: Epoch 006 - training loss: 1.3653, validating loss: 1.0760\n",
            "2024-03-18 06:23:28 [INFO]: Epoch 007 - training loss: 1.3653, validating loss: 1.0769\n",
            "2024-03-18 06:23:38 [INFO]: Epoch 008 - training loss: 1.3640, validating loss: 1.0746\n",
            "2024-03-18 06:23:47 [INFO]: Epoch 009 - training loss: 1.3639, validating loss: 1.0835\n",
            "2024-03-18 06:23:57 [INFO]: Epoch 010 - training loss: 1.3637, validating loss: 1.0751\n",
            "2024-03-18 06:23:57 [INFO]: Finished training.\n",
            "2024-03-18 06:23:57 [INFO]: Saved the model to tutorial_results/imputation/transformer/20240318_T062217/Transformer.pypots\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing mean absolute error: 0.6741\n"
          ]
        }
      ],
      "source": [
        "from pypots.optim import Adam\n",
        "from pypots.imputation import Transformer\n",
        "from pypots.utils.metrics import calc_mae\n",
        "\n",
        "# initialize the model\n",
        "transformer = Transformer(\n",
        "    n_steps=physionet2012_dataset['n_steps'],\n",
        "    n_features=physionet2012_dataset['n_features'],\n",
        "    n_layers=6,\n",
        "    d_model=512,\n",
        "    d_ffn=256,\n",
        "    n_heads=4,\n",
        "    d_k=128,\n",
        "    d_v=128,\n",
        "    dropout=0.1,\n",
        "    attn_dropout=0,\n",
        "    ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
        "    # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
        "    MIT_weight=1,\n",
        "    batch_size=32,\n",
        "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
        "    epochs=10,\n",
        "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
        "    # You can leave it to defualt as None to disable early stopping.\n",
        "    patience=3,\n",
        "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
        "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
        "    optimizer=Adam(lr=1e-3),\n",
        "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
        "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
        "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
        "    num_workers=0,\n",
        "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
        "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
        "    device=None,\n",
        "    # set the path for saving tensorboard and trained model files\n",
        "    saving_path=\"tutorial_results/imputation/transformer\",\n",
        "    # only save the best model after training finished.\n",
        "    # You can also set it as \"better\" to save models performing better ever during training.\n",
        "    model_saving_strategy=\"best\",\n",
        ")\n",
        "\n",
        "\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "transformer.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
        "\n",
        "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
        "transformer_results = transformer.predict(dataset_for_testing)\n",
        "transformer_imputation = transformer_results[\"imputation\"]\n",
        "\n",
        "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
        "testing_mae = calc_mae(\n",
        "    transformer_imputation,\n",
        "    physionet2012_dataset['test_X_ori'],\n",
        "    physionet2012_dataset['test_X_indicating_mask'],\n",
        ")\n",
        "print(f\"Testing mean absolute error: {testing_mae:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6In_IPwOc-g"
      },
      "source": [
        "### 🚀 An example of **TimesNet** for imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xytZs35XOjnT",
        "outputId": "20457086-7331-4716-d656-bafaa1c8b648"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-18 06:24:01 [INFO]: No given device, using default device: cuda\n",
            "2024-03-18 06:24:01 [INFO]: Model files will be saved to tutorial_results/imputation/timesnet/20240318_T062401\n",
            "2024-03-18 06:24:01 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/timesnet/20240318_T062401/tensorboard\n",
            "2024-03-18 06:24:02 [INFO]: TimesNet initialized with the given hyperparameters, the number of trainable parameters: 21,649,317\n",
            "2024-03-18 06:24:34 [INFO]: Epoch 001 - training loss: 0.4904, validating loss: 0.4605\n",
            "2024-03-18 06:25:06 [INFO]: Epoch 002 - training loss: 0.4393, validating loss: 0.4285\n",
            "2024-03-18 06:25:38 [INFO]: Epoch 003 - training loss: 0.3935, validating loss: 0.4192\n",
            "2024-03-18 06:26:09 [INFO]: Epoch 004 - training loss: 0.3980, validating loss: 0.4135\n",
            "2024-03-18 06:26:41 [INFO]: Epoch 005 - training loss: 0.3817, validating loss: 0.4081\n",
            "2024-03-18 06:27:13 [INFO]: Epoch 006 - training loss: 0.3760, validating loss: 0.4034\n",
            "2024-03-18 06:27:44 [INFO]: Epoch 007 - training loss: 0.3984, validating loss: 0.4004\n",
            "2024-03-18 06:28:16 [INFO]: Epoch 008 - training loss: 0.4220, validating loss: 0.3966\n",
            "2024-03-18 06:28:47 [INFO]: Epoch 009 - training loss: 0.4567, validating loss: 0.4001\n",
            "2024-03-18 06:29:19 [INFO]: Epoch 010 - training loss: 0.3817, validating loss: 0.3905\n",
            "2024-03-18 06:29:19 [INFO]: Finished training.\n",
            "2024-03-18 06:29:19 [INFO]: Saved the model to tutorial_results/imputation/timesnet/20240318_T062401/TimesNet.pypots\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing mean absolute error: 0.3276\n"
          ]
        }
      ],
      "source": [
        "from pypots.optim import Adam\n",
        "from pypots.imputation import TimesNet\n",
        "from pypots.utils.metrics import calc_mae\n",
        "\n",
        "# initialize the model\n",
        "timesnet = TimesNet(\n",
        "    n_steps=physionet2012_dataset['n_steps'],\n",
        "    n_features=physionet2012_dataset['n_features'],\n",
        "    n_layers=1,\n",
        "    top_k=1,\n",
        "    d_model=128,\n",
        "    d_ffn=512,\n",
        "    n_kernels=5,\n",
        "    dropout=0.5,\n",
        "    apply_nonstationary_norm=False,\n",
        "    batch_size=32,\n",
        "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
        "    epochs=10,\n",
        "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
        "    # You can leave it to defualt as None to disable early stopping.\n",
        "    patience=3,\n",
        "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
        "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
        "    optimizer=Adam(lr=1e-3),\n",
        "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
        "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
        "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
        "    num_workers=0,\n",
        "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
        "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
        "    device=None,\n",
        "    # set the path for saving tensorboard and trained model files\n",
        "    saving_path=\"tutorial_results/imputation/timesnet\",\n",
        "    # only save the best model after training finished.\n",
        "    # You can also set it as \"better\" to save models performing better ever during training.\n",
        "    model_saving_strategy=\"best\",\n",
        ")\n",
        "\n",
        "\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "timesnet.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
        "\n",
        "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
        "timesnet_results = timesnet.predict(dataset_for_testing)\n",
        "timesnet_imputation = timesnet_results[\"imputation\"]\n",
        "\n",
        "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
        "testing_mae = calc_mae(\n",
        "    timesnet_imputation,\n",
        "    physionet2012_dataset['test_X_ori'],\n",
        "    physionet2012_dataset['test_X_indicating_mask'],\n",
        ")\n",
        "print(f\"Testing mean absolute error: {testing_mae:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDD1__JXOffB"
      },
      "source": [
        "### 🚀 An example of **CSDI** for imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9HnXB8QO0Zp",
        "outputId": "b0d29b78-8ec0-41ef-f16c-1bc0a309da82"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-18 06:30:49 [INFO]: No given device, using default device: cuda\n",
            "2024-03-18 06:30:49 [INFO]: Model files will be saved to tutorial_results/imputation/csdi/20240318_T063049\n",
            "2024-03-18 06:30:49 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/csdi/20240318_T063049/tensorboard\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
            "2024-03-18 06:30:49 [INFO]: CSDI initialized with the given hyperparameters, the number of trainable parameters: 1,694,753\n",
            "2024-03-18 06:43:37 [INFO]: Epoch 001 - training loss: 0.3269, validating loss: 0.2420\n",
            "2024-03-18 06:56:24 [INFO]: Epoch 002 - training loss: 0.2617, validating loss: 0.2194\n",
            "2024-03-18 07:09:12 [INFO]: Epoch 003 - training loss: 0.2467, validating loss: 0.2036\n",
            "2024-03-18 07:21:59 [INFO]: Epoch 004 - training loss: 0.2390, validating loss: 0.1972\n",
            "2024-03-18 07:34:47 [INFO]: Epoch 005 - training loss: 0.2342, validating loss: 0.1972\n",
            "2024-03-18 07:47:34 [INFO]: Epoch 006 - training loss: 0.2308, validating loss: 0.1955\n",
            "2024-03-18 08:00:22 [INFO]: Epoch 007 - training loss: 0.2281, validating loss: 0.1948\n",
            "2024-03-18 08:13:09 [INFO]: Epoch 008 - training loss: 0.2299, validating loss: 0.1897\n",
            "2024-03-18 08:25:56 [INFO]: Epoch 009 - training loss: 0.2309, validating loss: 0.1870\n",
            "2024-03-18 08:38:43 [INFO]: Epoch 010 - training loss: 0.2278, validating loss: 0.1871\n",
            "2024-03-18 08:38:43 [INFO]: Finished training.\n",
            "2024-03-18 08:38:43 [INFO]: Saved the model to tutorial_results/imputation/csdi/20240318_T063049/CSDI.pypots\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shape of csdi_imputation is (2398, 2, 48, 37)\n",
            "Testing mean absolute error: 0.2788\n"
          ]
        }
      ],
      "source": [
        "from pypots.optim import Adam\n",
        "from pypots.imputation import CSDI\n",
        "from pypots.utils.metrics import calc_mae\n",
        "\n",
        "# initialize the model\n",
        "csdi = CSDI(\n",
        "    n_features=physionet2012_dataset['n_features'],\n",
        "    n_layers=6,\n",
        "    n_heads=2,\n",
        "    n_channels=128,\n",
        "    d_time_embedding=64,\n",
        "    d_feature_embedding=32,\n",
        "    d_diffusion_embedding=128,\n",
        "    target_strategy=\"random\",\n",
        "    n_diffusion_steps=50,\n",
        "    batch_size=32,\n",
        "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
        "    epochs=10,\n",
        "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
        "    # You can leave it to defualt as None to disable early stopping.\n",
        "    patience=3,\n",
        "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
        "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
        "    optimizer=Adam(lr=1e-3),\n",
        "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
        "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
        "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
        "    num_workers=0,\n",
        "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
        "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
        "    device=None,\n",
        "    # set the path for saving tensorboard and trained model files\n",
        "    saving_path=\"tutorial_results/imputation/csdi\",\n",
        "    # only save the best model after training finished.\n",
        "    # You can also set it as \"better\" to save models performing better ever during training.\n",
        "    model_saving_strategy=\"best\",\n",
        ")\n",
        "\n",
        "\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "csdi.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
        "\n",
        "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
        "\n",
        "# CSDI has an argument to control the number of sampling times during inference\n",
        "csdi_results = csdi.predict(dataset_for_testing, n_sampling_times=2)\n",
        "csdi_imputation = csdi_results[\"imputation\"]\n",
        "\n",
        "print(f\"The shape of csdi_imputation is {csdi_imputation.shape}\")\n",
        "\n",
        "# for error calculation, we need to take the mean value of the multiple samplings for each data sample\n",
        "mean_csdi_imputation = csdi_imputation.mean(axis=1)\n",
        "\n",
        "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
        "testing_mae = calc_mae(\n",
        "    mean_csdi_imputation,\n",
        "    physionet2012_dataset['test_X_ori'],\n",
        "    physionet2012_dataset['test_X_indicating_mask'],\n",
        ")\n",
        "print(f\"Testing mean absolute error: {testing_mae:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYMR2zc-XLEZ"
      },
      "source": [
        "### 🚀 An example of **US-GAN** for imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a0Fee8IXN-3",
        "outputId": "ac19ba9d-3d8e-48fa-8adc-470c9b086514"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-18 09:04:45 [INFO]: No given device, using default device: cuda\n",
            "2024-03-18 09:04:45 [INFO]: Model files will be saved to tutorial_results/imputation/us_gan/20240318_T090445\n",
            "2024-03-18 09:04:45 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/us_gan/20240318_T090445/tensorboard\n",
            "2024-03-18 09:04:45 [INFO]: USGAN initialized with the given hyperparameters, the number of trainable parameters: 1,258,517\n",
            "2024-03-18 09:07:27 [INFO]: Epoch 001 - generator training loss: 4.0775, discriminator training loss: 0.1833, validating loss: 0.4617\n",
            "2024-03-18 09:09:46 [INFO]: Epoch 002 - generator training loss: 4.8206, discriminator training loss: 0.1190, validating loss: 0.4199\n",
            "2024-03-18 09:12:04 [INFO]: Epoch 003 - generator training loss: 5.2968, discriminator training loss: 0.0918, validating loss: 0.4084\n",
            "2024-03-18 09:14:21 [INFO]: Epoch 004 - generator training loss: 5.6666, discriminator training loss: 0.0766, validating loss: 0.4003\n",
            "2024-03-18 09:16:40 [INFO]: Epoch 005 - generator training loss: 5.9683, discriminator training loss: 0.0668, validating loss: 0.3996\n",
            "2024-03-18 09:18:59 [INFO]: Epoch 006 - generator training loss: 6.2261, discriminator training loss: 0.0598, validating loss: 0.3949\n",
            "2024-03-18 09:21:14 [INFO]: Epoch 007 - generator training loss: 6.4520, discriminator training loss: 0.0545, validating loss: 0.3946\n",
            "2024-03-18 09:23:28 [INFO]: Epoch 008 - generator training loss: 6.6527, discriminator training loss: 0.0503, validating loss: 0.3931\n",
            "2024-03-18 09:25:44 [INFO]: Epoch 009 - generator training loss: 6.8366, discriminator training loss: 0.0468, validating loss: 0.3968\n"
          ]
        }
      ],
      "source": [
        "from pypots.optim import Adam\n",
        "from pypots.imputation import USGAN\n",
        "from pypots.utils.metrics import calc_mae\n",
        "\n",
        "# initialize the model\n",
        "us_gan = USGAN(\n",
        "    n_steps=physionet2012_dataset['n_steps'],\n",
        "    n_features=physionet2012_dataset['n_features'],\n",
        "    rnn_hidden_size=256,\n",
        "    lambda_mse=1,\n",
        "    dropout=0.1,\n",
        "    G_steps=1,\n",
        "    D_steps=1,\n",
        "    batch_size=32,\n",
        "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
        "    epochs=10,\n",
        "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
        "    # You can leave it to defualt as None to disable early stopping.\n",
        "    patience=3,\n",
        "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
        "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
        "    G_optimizer=Adam(lr=1e-3),\n",
        "    D_optimizer=Adam(lr=1e-3),\n",
        "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
        "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
        "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
        "    num_workers=0,\n",
        "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
        "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
        "    device=None,\n",
        "    # set the path for saving tensorboard and trained model files\n",
        "    saving_path=\"tutorial_results/imputation/us_gan\",\n",
        "    # only save the best model after training finished.\n",
        "    # You can also set it as \"better\" to save models performing better ever during training.\n",
        "    model_saving_strategy=\"best\",\n",
        ")\n",
        "\n",
        "\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "us_gan.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
        "\n",
        "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
        "us_gan_results = us_gan.predict(dataset_for_testing)\n",
        "us_gan_imputation = us_gan_results[\"imputation\"]\n",
        "\n",
        "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
        "testing_mae = calc_mae(\n",
        "    us_gan_imputation,\n",
        "    physionet2012_dataset['test_X_ori'],\n",
        "    physionet2012_dataset['test_X_indicating_mask'],\n",
        ")\n",
        "print(f\"Testing mean absolute error: {testing_mae:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu0i4NwXXXuG"
      },
      "source": [
        "### 🚀 An example of **GP-VAE** for imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iv-1DSSCXaS-",
        "outputId": "34b7b387-a93b-41f5-8a90-f48ca7321143"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-18 09:33:42 [INFO]: No given device, using default device: cpu\n",
            "2024-03-18 09:33:42 [INFO]: Model files will be saved to tutorial_results/imputation/gp_vae/20240318_T093342\n",
            "2024-03-18 09:33:42 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/gp_vae/20240318_T093342/tensorboard\n",
            "2024-03-18 09:33:42 [INFO]: GPVAE initialized with the given hyperparameters, the number of trainable parameters: 229,652\n",
            "2024-03-18 09:35:31 [INFO]: Epoch 001 - training loss: 26158.0052, validating loss: 0.7120\n",
            "2024-03-18 09:37:05 [INFO]: Epoch 002 - training loss: 22874.6371, validating loss: 0.6946\n",
            "2024-03-18 09:38:35 [INFO]: Epoch 003 - training loss: 22840.9400, validating loss: 0.6870\n",
            "2024-03-18 09:40:06 [INFO]: Epoch 004 - training loss: 22831.4853, validating loss: 0.6934\n",
            "2024-03-18 09:41:36 [INFO]: Epoch 005 - training loss: 22828.6990, validating loss: 0.6697\n",
            "2024-03-18 09:43:06 [INFO]: Epoch 006 - training loss: 22824.9689, validating loss: 0.6487\n",
            "2024-03-18 09:44:40 [INFO]: Epoch 007 - training loss: 22823.2533, validating loss: 0.6423\n",
            "2024-03-18 09:46:10 [INFO]: Epoch 008 - training loss: 22817.9086, validating loss: 0.5869\n",
            "2024-03-18 09:47:43 [INFO]: Epoch 009 - training loss: 22813.3121, validating loss: 0.7335\n",
            "2024-03-18 09:49:16 [INFO]: Epoch 010 - training loss: 22813.6369, validating loss: 0.6231\n",
            "2024-03-18 09:49:16 [INFO]: Finished training.\n",
            "2024-03-18 09:49:16 [INFO]: Saved the model to tutorial_results/imputation/gp_vae/20240318_T093342/GPVAE.pypots\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shape of gp_vae_imputation is (2398, 2, 48, 37)\n",
            "Testing mean absolute error: 0.4900\n"
          ]
        }
      ],
      "source": [
        "from pypots.optim import Adam\n",
        "from pypots.imputation import GPVAE\n",
        "from pypots.utils.metrics import calc_mae\n",
        "\n",
        "\n",
        "# initialize the model\n",
        "gp_vae = GPVAE(\n",
        "    n_steps=physionet2012_dataset['n_steps'],\n",
        "    n_features=physionet2012_dataset['n_features'],\n",
        "    latent_size=37,\n",
        "    encoder_sizes=(128,128),\n",
        "    decoder_sizes=(256,256),\n",
        "    kernel=\"cauchy\",\n",
        "    beta=0.2,\n",
        "    M=1,\n",
        "    K=1,\n",
        "    sigma=1.005,\n",
        "    length_scale=7.0,\n",
        "    kernel_scales=1,\n",
        "    window_size=24,\n",
        "    batch_size=32,\n",
        "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
        "    epochs=10,\n",
        "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
        "    # You can leave it to defualt as None to disable early stopping.\n",
        "    patience=3,\n",
        "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
        "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
        "    optimizer=Adam(lr=1e-3),\n",
        "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
        "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
        "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
        "    num_workers=0,\n",
        "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
        "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
        "    device=None,\n",
        "    # set the path for saving tensorboard and trained model files\n",
        "    saving_path=\"tutorial_results/imputation/gp_vae\",\n",
        "    # only save the best model after training finished.\n",
        "    # You can also set it as \"better\" to save models performing better ever during training.\n",
        "    model_saving_strategy=\"best\",\n",
        ")\n",
        "\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "gp_vae.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
        "\n",
        "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
        "\n",
        "# GP-VAE has an argument to control the number of sampling times during inference\n",
        "gp_vae_results = gp_vae.predict(dataset_for_testing, n_sampling_times=2)\n",
        "gp_vae_imputation = gp_vae_results[\"imputation\"]\n",
        "\n",
        "print(f\"The shape of gp_vae_imputation is {gp_vae_imputation.shape}\")\n",
        "\n",
        "# for error calculation, we need to take the mean value of the multiple samplings for each data sample\n",
        "mean_gp_vae_imputation = gp_vae_imputation.mean(axis=1)\n",
        "\n",
        "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
        "testing_mae = calc_mae(\n",
        "    mean_gp_vae_imputation,\n",
        "    physionet2012_dataset['test_X_ori'],\n",
        "    physionet2012_dataset['test_X_indicating_mask'],\n",
        ")\n",
        "print(f\"Testing mean absolute error: {testing_mae:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGL2QaXl-nm3"
      },
      "source": [
        "### 🚀 An example of **BRITS** for imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tgbgou-_-qMl",
        "outputId": "bd94e0d7-c9ac-4815-f941-43a6bf6cb643"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-18 09:49:33 [INFO]: No given device, using default device: cpu\n",
            "2024-03-18 09:49:33 [INFO]: Model files will be saved to tutorial_results/imputation/brits/20240318_T094933\n",
            "2024-03-18 09:49:33 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/brits/20240318_T094933/tensorboard\n",
            "2024-03-18 09:49:33 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 239,344\n",
            "2024-03-18 09:51:21 [INFO]: Epoch 001 - training loss: 0.9293, validating loss: 0.4721\n",
            "2024-03-18 09:52:39 [INFO]: Epoch 002 - training loss: 0.7275, validating loss: 0.4292\n",
            "2024-03-18 09:54:10 [INFO]: Epoch 003 - training loss: 0.6797, validating loss: 0.4139\n",
            "2024-03-18 09:55:35 [INFO]: Epoch 004 - training loss: 0.6552, validating loss: 0.4082\n",
            "2024-03-18 09:56:57 [INFO]: Epoch 005 - training loss: 0.6406, validating loss: 0.4050\n",
            "2024-03-18 09:58:22 [INFO]: Epoch 006 - training loss: 0.6292, validating loss: 0.4024\n",
            "2024-03-18 09:59:45 [INFO]: Epoch 007 - training loss: 0.6201, validating loss: 0.4032\n",
            "2024-03-18 10:01:10 [INFO]: Epoch 008 - training loss: 0.6127, validating loss: 0.4034\n",
            "2024-03-18 10:02:34 [INFO]: Epoch 009 - training loss: 0.6059, validating loss: 0.4047\n",
            "2024-03-18 10:02:34 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
            "2024-03-18 10:02:34 [INFO]: Finished training.\n",
            "2024-03-18 10:02:34 [INFO]: Saved the model to tutorial_results/imputation/brits/20240318_T094933/BRITS.pypots\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing mean absolute error: 0.2546\n"
          ]
        }
      ],
      "source": [
        "from pypots.optim import Adam\n",
        "from pypots.imputation import BRITS\n",
        "from pypots.utils.metrics import calc_mae\n",
        "\n",
        "# initialize the model\n",
        "brits = BRITS(\n",
        "    n_steps=physionet2012_dataset['n_steps'],\n",
        "    n_features=physionet2012_dataset['n_features'],\n",
        "    rnn_hidden_size=128,\n",
        "    batch_size=32,\n",
        "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
        "    epochs=10,\n",
        "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
        "    # You can leave it to defualt as None to disable early stopping.\n",
        "    patience=3,\n",
        "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
        "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
        "    optimizer=Adam(lr=1e-3),\n",
        "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
        "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
        "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
        "    num_workers=0,\n",
        "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
        "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
        "    device=None,\n",
        "    # set the path for saving tensorboard and trained model files\n",
        "    saving_path=\"tutorial_results/imputation/brits\",\n",
        "    # only save the best model after training finished.\n",
        "    # You can also set it as \"better\" to save models performing better ever during training.\n",
        "    model_saving_strategy=\"best\",\n",
        ")\n",
        "\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "brits.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
        "\n",
        "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
        "brits_results = brits.predict(dataset_for_testing)\n",
        "brits_imputation = brits_results[\"imputation\"]\n",
        "\n",
        "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
        "testing_mae = calc_mae(\n",
        "    brits_imputation,\n",
        "    physionet2012_dataset['test_X_ori'],\n",
        "    physionet2012_dataset['test_X_indicating_mask'],\n",
        ")\n",
        "print(f\"Testing mean absolute error: {testing_mae:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPw7U1AgYmbK"
      },
      "source": [
        "### 🚀 An example of **M-RNN** for imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "gruCE389YpF-",
        "outputId": "7b5f267e-cd7e-46c0-b1e4-8029e5d11234"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-18 10:02:51 [INFO]: No given device, using default device: cpu\n",
            "2024-03-18 10:02:51 [INFO]: Model files will be saved to tutorial_results/imputation/mrnn/20240318_T100251\n",
            "2024-03-18 10:02:51 [INFO]: Tensorboard file will be saved to tutorial_results/imputation/mrnn/20240318_T100251/tensorboard\n",
            "2024-03-18 10:02:51 [INFO]: MRNN initialized with the given hyperparameters, the number of trainable parameters: 107,951\n",
            "2024-03-18 10:13:26 [INFO]: Epoch 001 - training loss: 0.7622, validating loss: 1.0173\n",
            "2024-03-18 10:23:22 [INFO]: Epoch 002 - training loss: 0.5198, validating loss: 0.9760\n",
            "2024-03-18 10:33:24 [INFO]: Epoch 003 - training loss: 0.4850, validating loss: 0.9627\n",
            "2024-03-18 10:43:35 [INFO]: Epoch 004 - training loss: 0.4666, validating loss: 0.9571\n",
            "2024-03-18 10:53:35 [INFO]: Epoch 005 - training loss: 0.4482, validating loss: 0.9585\n",
            "2024-03-18 11:03:38 [INFO]: Epoch 006 - training loss: 0.4405, validating loss: 0.9522\n",
            "2024-03-18 11:13:40 [INFO]: Epoch 007 - training loss: 0.4350, validating loss: 0.9548\n",
            "2024-03-18 11:23:45 [INFO]: Epoch 008 - training loss: 0.4322, validating loss: 0.9574\n",
            "2024-03-18 11:33:39 [INFO]: Epoch 009 - training loss: 0.4223, validating loss: 0.9600\n",
            "2024-03-18 11:33:39 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
            "2024-03-18 11:33:39 [INFO]: Finished training.\n",
            "2024-03-18 11:33:39 [INFO]: Saved the model to tutorial_results/imputation/mrnn/20240318_T100251/MRNN.pypots\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing mean absolute error: 0.6699\n"
          ]
        }
      ],
      "source": [
        "from pypots.optim import Adam\n",
        "from pypots.imputation import MRNN\n",
        "from pypots.utils.metrics import calc_mae\n",
        "\n",
        "# initialize the model\n",
        "# initialize the model\n",
        "mrnn = MRNN(\n",
        "    n_steps=physionet2012_dataset['n_steps'],\n",
        "    n_features=physionet2012_dataset['n_features'],\n",
        "    rnn_hidden_size=128,\n",
        "\n",
        "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
        "    epochs=10,\n",
        "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
        "    # You can leave it to defualt as None to disable early stopping.\n",
        "    patience=3,\n",
        "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
        "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
        "    optimizer=Adam(lr=1e-3),\n",
        "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
        "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
        "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
        "    num_workers=0,\n",
        "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
        "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
        "    device=None,\n",
        "    # set the path for saving tensorboard and trained model files\n",
        "    saving_path=\"tutorial_results/imputation/mrnn\",\n",
        "    # only save the best model after training finished.\n",
        "    # You can also set it as \"better\" to save models performing better ever during training.\n",
        "    model_saving_strategy=\"best\",\n",
        ")\n",
        "\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "mrnn.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
        "\n",
        "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
        "mrnn_results = mrnn.predict(dataset_for_testing)\n",
        "mrnn_imputation = mrnn_results[\"imputation\"]\n",
        "\n",
        "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
        "testing_mae = calc_mae(\n",
        "    mrnn_imputation,\n",
        "    physionet2012_dataset['test_X_ori'],\n",
        "    physionet2012_dataset['test_X_indicating_mask'],\n",
        ")\n",
        "print(f\"Testing mean absolute error: {testing_mae:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14XhW8yN-3gl"
      },
      "source": [
        "### 🚀 An example of **LOCF** for imputation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qZS5aP2j-5TE",
        "outputId": "2c193c54-0d96-4e25-a9e8-d6d362402921"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-18 11:34:37 [INFO]: No given device, using default device: cpu\n",
            "/usr/local/lib/python3.10/dist-packages/pypots/imputation/locf/model.py:66: UserWarning: LOCF (Last Observed Carried Forward) imputation class has no parameter to train. Please run func `predict()` directly.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing mean absolute error: 0.4112\n"
          ]
        }
      ],
      "source": [
        "from pypots.imputation import LOCF\n",
        "from pypots.utils.metrics import cal_mae\n",
        "\n",
        "from pypots.imputation import LOCF\n",
        "\n",
        "# initialize the model\n",
        "locf = LOCF()\n",
        "\n",
        "# LOCF doesn't need to be trained, just call the impute() function\n",
        "locf.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
        "\n",
        "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
        "locf_results = locf.predict(dataset_for_testing)\n",
        "locf_imputation = locf_results[\"imputation\"]\n",
        "\n",
        "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
        "testing_mae = calc_mae(\n",
        "    locf_imputation,\n",
        "    physionet2012_dataset['test_X_ori'],\n",
        "    physionet2012_dataset['test_X_indicating_mask'],\n",
        ")\n",
        "print(f\"Testing mean absolute error: {testing_mae:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4bU3eZY_-01"
      },
      "source": [
        "## 🌟 Clustering Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "udnvIVuUADBU"
      },
      "outputs": [],
      "source": [
        "# Assemble the datasets for training, validating, and testing.\n",
        "import numpy as np\n",
        "\n",
        "# don't need validation set\n",
        "dataset_for_training = {\n",
        "    \"X\": np.concatenate([physionet2012_dataset['train_X'], physionet2012_dataset['val_X']], axis=0),\n",
        "    \"y\": np.concatenate([physionet2012_dataset['train_y'], physionet2012_dataset['val_y']], axis=0),\n",
        "}\n",
        "\n",
        "dataset_for_testing = {\n",
        "    \"X\": physionet2012_dataset['test_X'],\n",
        "    \"y\": physionet2012_dataset['test_y'],\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvyVryEwAEEk"
      },
      "source": [
        "### 🚀 An example of **CRLI** for clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjdHa-fiAJbP",
        "outputId": "1137fa5c-eaa0-4826-c5c5-53afb1d0a31a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-03-18 11:34:38 [INFO]: No given device, using default device: cpu\n",
            "2024-03-18 11:34:38 [INFO]: Model files will be saved to tutorial_results/clustering/crli/20240318_T113438\n",
            "2024-03-18 11:34:38 [INFO]: Tensorboard file will be saved to tutorial_results/clustering/crli/20240318_T113438/tensorboard\n",
            "2024-03-18 11:34:38 [INFO]: CRLI initialized with the given hyperparameters, the number of trainable parameters: 1,546,820\n",
            "2024-03-18 11:40:35 [INFO]: Epoch 001 - generator training loss: 3.1785, discriminator training loss: 0.3930\n",
            "2024-03-18 11:46:28 [INFO]: Epoch 002 - generator training loss: 3.2566, discriminator training loss: 0.3703\n",
            "2024-03-18 11:52:20 [INFO]: Epoch 003 - generator training loss: 3.2419, discriminator training loss: 0.3625\n",
            "2024-03-18 11:58:09 [INFO]: Epoch 004 - generator training loss: 3.3570, discriminator training loss: 0.3584\n",
            "2024-03-18 11:58:09 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
            "2024-03-18 11:58:09 [INFO]: Finished training.\n",
            "2024-03-18 11:58:09 [INFO]: Saved the model to tutorial_results/clustering/crli/20240318_T113438/CRLI.pypots\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing clustering metrics: \n",
            "RI: 0.6756857943432906, \n",
            "CP: 0.8486238532110092\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pypots.optim import Adam\n",
        "from pypots.clustering import CRLI\n",
        "from pypots.utils.metrics import calc_rand_index, calc_cluster_purity\n",
        "\n",
        "# initialize the model\n",
        "crli = CRLI(\n",
        "    n_steps=physionet2012_dataset[\"n_steps\"],\n",
        "    n_features=physionet2012_dataset[\"n_features\"],\n",
        "    n_clusters=physionet2012_dataset[\"n_classes\"],\n",
        "    n_generator_layers=2,\n",
        "    rnn_hidden_size=256,\n",
        "    rnn_cell_type=\"GRU\",\n",
        "    decoder_fcn_output_dims=[256, 128],  # the output dimensions of layers in the decoder FCN.\n",
        "    # Here means there are 3 layers. Leave it to default as None will results in\n",
        "    # the FCN haveing only one layer.\n",
        "    batch_size=32,\n",
        "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
        "    epochs=10,\n",
        "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
        "    # You can leave it to defualt as None to disable early stopping.\n",
        "    patience=3,\n",
        "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
        "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
        "    G_optimizer=Adam(lr=1e-3),\n",
        "    D_optimizer=Adam(lr=1e-3),\n",
        "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
        "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
        "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
        "    num_workers=0,\n",
        "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
        "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
        "    device=None,\n",
        "    # set the path for saving tensorboard and trained model files\n",
        "    saving_path=\"tutorial_results/clustering/crli\",\n",
        "    # only save the best model after training finished.\n",
        "    # You can also set it as \"better\" to save models performing better ever during training.\n",
        "    model_saving_strategy=\"best\",\n",
        ")\n",
        "\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "crli.fit(train_set=dataset_for_training)\n",
        "\n",
        "# the testing stage\n",
        "crli_results = crli.predict(dataset_for_testing)\n",
        "crli_prediction = crli_results[\"clustering\"]\n",
        "\n",
        "# calculate the values of clustering metrics on the model's prediction\n",
        "RI = calc_rand_index(crli_prediction, dataset_for_testing[\"y\"])\n",
        "CP = calc_cluster_purity(crli_prediction, dataset_for_testing[\"y\"])\n",
        "\n",
        "print(\"Testing clustering metrics: \\n\"\n",
        "      f'RI: {RI}, \\n'\n",
        "      f'CP: {CP}\\n'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhoovboEATo7"
      },
      "source": [
        "### 🚀 An example of **VaDER** for clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HN9kmDtoAWuk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d34d94b9-2987-43a3-ba79-2e1c63c00e15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-18 11:58:23 [INFO]: No given device, using default device: cpu\n",
            "2024-03-18 11:58:23 [INFO]: Model files will be saved to tutorial_results/clustering/vader/20240318_T115823\n",
            "2024-03-18 11:58:23 [INFO]: Tensorboard file will be saved to tutorial_results/clustering/vader/20240318_T115823/tensorboard\n",
            "2024-03-18 11:58:23 [INFO]: VaDER initialized with the given hyperparameters, the number of trainable parameters: 293,644\n",
            "2024-03-18 12:25:14 [INFO]: Epoch 001 - training loss: 0.5046\n",
            "2024-03-18 12:27:21 [INFO]: Epoch 002 - training loss: 0.2418\n",
            "2024-03-18 12:29:42 [INFO]: Epoch 003 - training loss: 0.2420\n",
            "2024-03-18 12:32:06 [INFO]: Epoch 004 - training loss: 0.2274\n",
            "2024-03-18 12:34:31 [INFO]: Epoch 005 - training loss: 0.2373\n",
            "2024-03-18 12:37:11 [INFO]: Epoch 006 - training loss: 0.2363\n",
            "2024-03-18 12:39:54 [INFO]: Epoch 007 - training loss: 0.2302\n",
            "2024-03-18 12:39:54 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
            "2024-03-18 12:39:54 [INFO]: Finished training.\n",
            "2024-03-18 12:39:54 [INFO]: Saved the model to tutorial_results/clustering/vader/20240318_T115823/VaDER.pypots\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing clustering metrics: \n",
            "RI: 0.7429699968997945, \n",
            "CP: 0.8486238532110092,\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pypots.optim import Adam\n",
        "from pypots.clustering import VaDER\n",
        "from pypots.utils.metrics import calc_rand_index, calc_cluster_purity\n",
        "\n",
        "# initialize the model\n",
        "vader = VaDER(\n",
        "    n_steps=physionet2012_dataset[\"n_steps\"],\n",
        "    n_features=physionet2012_dataset[\"n_features\"],\n",
        "    n_clusters=physionet2012_dataset[\"n_classes\"],\n",
        "    rnn_hidden_size=128,\n",
        "    d_mu_stddev=2,\n",
        "    pretrain_epochs=20,\n",
        "    batch_size=32,\n",
        "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
        "    epochs=10,\n",
        "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
        "    # You can leave it to defualt as None to disable early stopping.\n",
        "    patience=3,\n",
        "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
        "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
        "    optimizer=Adam(lr=1e-3),\n",
        "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
        "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
        "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
        "    num_workers=0,\n",
        "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
        "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
        "    device=None,\n",
        "    # set the path for saving tensorboard and trained model files\n",
        "    saving_path=\"tutorial_results/clustering/vader\",\n",
        "    # only save the best model after training finished.\n",
        "    # You can also set it as \"better\" to save models performing better ever during training.\n",
        "    model_saving_strategy=\"best\",\n",
        ")\n",
        "\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "vader.fit(train_set=dataset_for_training)\n",
        "\n",
        "# the testing stage\n",
        "vader_results = vader.predict(dataset_for_testing)\n",
        "vader_prediction = vader_results[\"clustering\"]\n",
        "\n",
        "# calculate the values of clustering metrics on the model's prediction\n",
        "RI = calc_rand_index(vader_prediction, dataset_for_testing[\"y\"])\n",
        "CP = calc_cluster_purity(vader_prediction, dataset_for_testing[\"y\"])\n",
        "\n",
        "print(\"Testing clustering metrics: \\n\"\n",
        "      f'RI: {RI}, \\n'\n",
        "      f'CP: {CP},\\n'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoR-KkgtAiZx"
      },
      "source": [
        "## 🌟 Forecasting Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvsTR8ARAh5P"
      },
      "outputs": [],
      "source": [
        "# Assemble the datasets for training, validating, and testing.\n",
        "\n",
        "dataset_for_training = {\n",
        "    \"X\": physionet2012_dataset['train_X'],\n",
        "}\n",
        "\n",
        "dataset_for_validating = {\n",
        "    \"X\": physionet2012_dataset['val_X'],\n",
        "    \"X_intact\": physionet2012_dataset['val_X_ori'],\n",
        "}\n",
        "\n",
        "dataset_for_testing = {\n",
        "    \"X\": physionet2012_dataset['test_X'][:, :36],  # we only take the first 36 steps for model input,\n",
        "    # and let the model forecast the left 12 steps\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVIB7HhdAn2V"
      },
      "source": [
        "### 🚀 An example of **BTTF** for forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brxb6kAoAq6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "652f63a4-f8d6-4a76-c885-935fc88c6982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-18 12:40:01 [INFO]: No given device, using default device: cpu\n",
            "2024-03-18 12:40:01 [WARNING]: ‼️ saving_path not given. Model files and tensorboard file will not be saved.\n",
            "/usr/local/lib/python3.10/dist-packages/pypots/forecasting/bttf/model.py:117: UserWarning: Please run func forecast(X) directly.\n",
            "  warnings.warn(\"Please run func forecast(X) directly.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing mean absolute error: 0.8055\n"
          ]
        }
      ],
      "source": [
        "from pypots.forecasting import BTTF\n",
        "from pypots.utils.metrics import calc_mae\n",
        "\n",
        "# initialize the model\n",
        "bttf = BTTF(\n",
        "    36,\n",
        "    physionet2012_dataset[\"n_features\"],\n",
        "    pred_step=12,\n",
        "    rank=10,\n",
        "    time_lags=[1, 2, 3, 10, 10 + 1, 10 + 2, 20, 20 + 1, 20 + 2],\n",
        "    burn_iter=5,\n",
        "    gibbs_iter=5,\n",
        "    multi_step=1,\n",
        ")\n",
        "\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "bttf.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
        "# BTTF does not need to run func fits().\n",
        "\n",
        "# the testing stage\n",
        "bttf_results = bttf.predict(dataset_for_testing)\n",
        "bttf_prediction = bttf_results[\"forecasting\"]\n",
        "\n",
        "# calculate the mean absolute error on the ground truth in the forecasting task\n",
        "testing_mae = calc_mae(\n",
        "    bttf_prediction,\n",
        "    np.nan_to_num(physionet2012_dataset['test_X'][:, 36:]),\n",
        "    (~np.isnan(physionet2012_dataset['test_X'][:, 36:])).astype(int),\n",
        ")\n",
        "print(f\"Testing mean absolute error: {testing_mae:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnNGplDi_H09"
      },
      "source": [
        "## 🌟 Classification Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hm5IMCa1-8WW"
      },
      "outputs": [],
      "source": [
        "# Assemble the datasets for training, validating, and testing.\n",
        "\n",
        "dataset_for_training = {\n",
        "    \"X\": physionet2012_dataset['train_X'],\n",
        "    \"y\": physionet2012_dataset['train_y'],\n",
        "}\n",
        "\n",
        "dataset_for_validating = {\n",
        "    \"X\": physionet2012_dataset['val_X'],\n",
        "    \"y\": physionet2012_dataset['val_y'],\n",
        "}\n",
        "\n",
        "dataset_for_testing = {\n",
        "    \"X\": physionet2012_dataset['test_X'],\n",
        "    \"y\": physionet2012_dataset['test_y'],\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlluYv6q_jMn"
      },
      "source": [
        "### 🚀 An example of **BRITS** for classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvxZl33v_jCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77859e61-8928-44c0-e445-634ae32c216c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-18 12:40:15 [INFO]: No given device, using default device: cpu\n",
            "2024-03-18 12:40:15 [INFO]: Model files will be saved to tutorial_results/classification/brits/20240318_T124015\n",
            "2024-03-18 12:40:15 [INFO]: Tensorboard file will be saved to tutorial_results/classification/brits/20240318_T124015/tensorboard\n",
            "2024-03-18 12:40:16 [INFO]: BRITS initialized with the given hyperparameters, the number of trainable parameters: 730,612\n",
            "2024-03-18 12:42:38 [INFO]: Epoch 001 - training loss: 0.9063, validating loss: 0.7915\n",
            "2024-03-18 12:44:33 [INFO]: Epoch 002 - training loss: 0.7632, validating loss: 0.7572\n",
            "2024-03-18 12:46:28 [INFO]: Epoch 003 - training loss: 0.7197, validating loss: 0.7349\n",
            "2024-03-18 12:48:23 [INFO]: Epoch 004 - training loss: 0.7089, validating loss: 0.7252\n",
            "2024-03-18 12:50:18 [INFO]: Epoch 005 - training loss: 0.6794, validating loss: 0.7184\n",
            "2024-03-18 12:52:14 [INFO]: Epoch 006 - training loss: 0.6765, validating loss: 0.7295\n",
            "2024-03-18 12:54:09 [INFO]: Epoch 007 - training loss: 0.6584, validating loss: 0.7113\n",
            "2024-03-18 12:56:04 [INFO]: Epoch 008 - training loss: 0.6441, validating loss: 0.7183\n",
            "2024-03-18 12:58:00 [INFO]: Epoch 009 - training loss: 0.6267, validating loss: 0.7095\n",
            "2024-03-18 12:59:56 [INFO]: Epoch 010 - training loss: 0.6039, validating loss: 0.7433\n",
            "2024-03-18 12:59:56 [INFO]: Finished training.\n",
            "2024-03-18 12:59:56 [INFO]: Saved the model to tutorial_results/classification/brits/20240318_T124015/BRITS.pypots\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing classification metrics: \n",
            "ROC_AUC: 0.8413832314658761, \n",
            "PR_AUC: 0.5156639586718953,\n",
            "F1: 0.4335664335664336,\n",
            "Precision: 0.5933014354066986,\n",
            "Recall: 0.3415977961432507,\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pypots.optim import Adam\n",
        "from pypots.classification import BRITS\n",
        "from pypots.utils.metrics import calc_binary_classification_metrics\n",
        "\n",
        "# initialize the model\n",
        "brits = BRITS(\n",
        "    n_steps=physionet2012_dataset['n_steps'],\n",
        "    n_features=physionet2012_dataset['n_features'],\n",
        "    n_classes=physionet2012_dataset[\"n_classes\"],\n",
        "    rnn_hidden_size=256,\n",
        "    batch_size=32,\n",
        "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
        "    epochs=10,\n",
        "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
        "    # You can leave it to defualt as None to disable early stopping.\n",
        "    patience=3,\n",
        "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
        "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
        "    optimizer=Adam(lr=1e-3),\n",
        "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
        "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
        "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
        "    num_workers=0,\n",
        "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
        "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
        "    device=None,\n",
        "    # set the path for saving tensorboard and trained model files\n",
        "    saving_path=\"tutorial_results/classification/brits\",\n",
        "    # only save the best model after training finished.\n",
        "    # You can also set it as \"better\" to save models performing better ever during training.\n",
        "    model_saving_strategy=\"best\",\n",
        ")\n",
        "\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "brits.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
        "\n",
        "# the testing stage\n",
        "brits_results = brits.predict(dataset_for_testing)\n",
        "brits_prediction = brits_results[\"classification\"]\n",
        "\n",
        "# calculate the values of binary classification metrics on the model's prediction\n",
        "metrics = calc_binary_classification_metrics(brits_prediction, dataset_for_testing[\"y\"])\n",
        "print(\"Testing classification metrics: \\n\"\n",
        "    f'ROC_AUC: {metrics[\"roc_auc\"]}, \\n'\n",
        "    f'PR_AUC: {metrics[\"pr_auc\"]},\\n'\n",
        "    f'F1: {metrics[\"f1\"]},\\n'\n",
        "    f'Precision: {metrics[\"precision\"]},\\n'\n",
        "    f'Recall: {metrics[\"recall\"]},\\n'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msmIoNGH_wQ4"
      },
      "source": [
        "### 🚀 An example of **GRUD** for classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-hLkYbR_vru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dba37c7-c366-4b65-8985-69efc2ea9c20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-18 13:00:14 [INFO]: No given device, using default device: cpu\n",
            "2024-03-18 13:00:14 [INFO]: Model files will be saved to tutorial_results/classification/grud/20240318_T130014\n",
            "2024-03-18 13:00:14 [INFO]: Tensorboard file will be saved to tutorial_results/classification/grud/20240318_T130014/tensorboard\n",
            "2024-03-18 13:00:14 [INFO]: GRUD initialized with the given hyperparameters, the number of trainable parameters: 16,128\n",
            "2024-03-18 13:00:14 [INFO]: No given device, using default device: cpu\n",
            "2024-03-18 13:00:27 [INFO]: No given device, using default device: cpu\n",
            "2024-03-18 13:00:46 [INFO]: Epoch 001 - training loss: 0.3497, validating loss: 0.3167\n",
            "2024-03-18 13:01:01 [INFO]: Epoch 002 - training loss: 0.3012, validating loss: 0.3019\n",
            "2024-03-18 13:01:18 [INFO]: Epoch 003 - training loss: 0.2901, validating loss: 0.3011\n",
            "2024-03-18 13:01:34 [INFO]: Epoch 004 - training loss: 0.2799, validating loss: 0.3026\n",
            "2024-03-18 13:01:49 [INFO]: Epoch 005 - training loss: 0.2694, validating loss: 0.2989\n",
            "2024-03-18 13:02:06 [INFO]: Epoch 006 - training loss: 0.2641, validating loss: 0.2959\n",
            "2024-03-18 13:02:22 [INFO]: Epoch 007 - training loss: 0.2582, validating loss: 0.3074\n",
            "2024-03-18 13:02:37 [INFO]: Epoch 008 - training loss: 0.2491, validating loss: 0.2993\n",
            "2024-03-18 13:02:54 [INFO]: Epoch 009 - training loss: 0.2415, validating loss: 0.3148\n",
            "2024-03-18 13:02:54 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
            "2024-03-18 13:02:54 [INFO]: Finished training.\n",
            "2024-03-18 13:02:54 [INFO]: Saved the model to tutorial_results/classification/grud/20240318_T130014/GRUD.pypots\n",
            "2024-03-18 13:02:54 [INFO]: No given device, using default device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing classification metrics: \n",
            "ROC_AUC: 0.8558301351689782, \n",
            "PR_AUC: 0.5432786552563934,\n",
            "F1: 0.5080906148867314,\n",
            "Precision: 0.615686274509804,\n",
            "Recall: 0.4325068870523416,\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pypots.optim import Adam\n",
        "from pypots.classification import GRUD\n",
        "from pypots.utils.metrics import calc_binary_classification_metrics\n",
        "\n",
        "# initialize the model\n",
        "grud = GRUD(\n",
        "    n_steps=physionet2012_dataset['n_steps'],\n",
        "    n_features=physionet2012_dataset['n_features'],\n",
        "    n_classes=physionet2012_dataset[\"n_classes\"],\n",
        "    rnn_hidden_size=32,\n",
        "    batch_size=32,\n",
        "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
        "    epochs=10,\n",
        "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
        "    # You can leave it to defualt as None to disable early stopping.\n",
        "    patience=3,\n",
        "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
        "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
        "    optimizer=Adam(lr=1e-3),\n",
        "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
        "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
        "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
        "    num_workers=0,\n",
        "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
        "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
        "    device=None,\n",
        "    # set the path for saving tensorboard and trained model files\n",
        "    saving_path=\"tutorial_results/classification/grud\",\n",
        "    # only save the best model after training finished.\n",
        "    # You can also set it as \"better\" to save models performing better ever during training.\n",
        "    model_saving_strategy=\"best\",\n",
        ")\n",
        "\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "grud.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
        "\n",
        "# the testing stage\n",
        "grud_results = grud.predict(dataset_for_testing)\n",
        "grud_prediction = grud_results[\"classification\"]\n",
        "\n",
        "# calculate the values of binary classification metrics on the model's prediction\n",
        "metrics = calc_binary_classification_metrics(grud_prediction, dataset_for_testing[\"y\"])\n",
        "print(\"Testing classification metrics: \\n\"\n",
        "    f'ROC_AUC: {metrics[\"roc_auc\"]}, \\n'\n",
        "    f'PR_AUC: {metrics[\"pr_auc\"]},\\n'\n",
        "    f'F1: {metrics[\"f1\"]},\\n'\n",
        "    f'Precision: {metrics[\"precision\"]},\\n'\n",
        "    f'Recall: {metrics[\"recall\"]},\\n'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1uXdzhY_Ypx"
      },
      "source": [
        "### 🚀 An example of **Raindrop** for classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iA49L0bF3vL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dd967b6-9509-4f30-c304-6657853d5f4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installed torch version: 2.2.1+cu121\n",
            "Now install necessary dependencies (pyg etc.) for the Raindrop model...\n",
            "\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.2.1+cpu.html\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.5.1)\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.10/dist-packages (2.1.2+pt22cu121)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.10/dist-packages (0.6.18+pt22cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.3.0)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(f\"Installed torch version: {torch.__version__}\")\n",
        "print(\"Now install necessary dependencies (pyg etc.) for the Raindrop model...\\n\")\n",
        "\n",
        "pyg_whl_link = f\"https://data.pyg.org/whl/torch-{torch.__version__}.html\"\n",
        "\n",
        "! pip install torch-geometric torch-scatter torch-sparse -f $pyg_whl_link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShuLMQ0L-9mr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93039b5b-2163-45f8-f6c7-0fa20f9507be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-03-19 09:41:48 [INFO]: No given device, using default device: cuda\n",
            "2024-03-19 09:41:48 [INFO]: Model files will be saved to tutorial_results/classification/raindrop/20240319_T094148\n",
            "2024-03-19 09:41:48 [INFO]: Tensorboard file will be saved to tutorial_results/classification/raindrop/20240319_T094148/tensorboard\n",
            "2024-03-19 09:41:48 [INFO]: Raindrop initialized with the given hyperparameters, the number of trainable parameters: 1,415,006\n",
            "2024-03-19 09:42:27 [INFO]: Epoch 001 - training loss: 0.3972, validating loss: 0.3448\n",
            "2024-03-19 09:42:49 [INFO]: Epoch 002 - training loss: 0.3358, validating loss: 0.3341\n",
            "2024-03-19 09:43:11 [INFO]: Epoch 003 - training loss: 0.3195, validating loss: 0.3501\n",
            "2024-03-19 09:43:32 [INFO]: Epoch 004 - training loss: 0.3095, validating loss: 0.3208\n",
            "2024-03-19 09:43:53 [INFO]: Epoch 005 - training loss: 0.3022, validating loss: 0.3282\n",
            "2024-03-19 09:44:14 [INFO]: Epoch 006 - training loss: 0.2933, validating loss: 0.3335\n",
            "2024-03-19 09:44:35 [INFO]: Epoch 007 - training loss: 0.2916, validating loss: 0.3356\n",
            "2024-03-19 09:44:35 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
            "2024-03-19 09:44:35 [INFO]: Finished training.\n",
            "2024-03-19 09:44:35 [INFO]: Saved the model to tutorial_results/classification/raindrop/20240319_T094148/Raindrop.pypots\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing classification metrics: \n",
            "ROC_AUC: 0.8470634421047645, \n",
            "PR_AUC: 0.5042968844353513,\n",
            "F1: 0.3080168776371308,\n",
            "Precision: 0.6576576576576577,\n",
            "Recall: 0.20110192837465565,\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pypots.optim import Adam\n",
        "from pypots.classification import Raindrop\n",
        "from pypots.utils.metrics import calc_binary_classification_metrics\n",
        "\n",
        "# initialize the model\n",
        "raindrop = Raindrop(\n",
        "    n_steps=physionet2012_dataset['n_steps'],\n",
        "    n_features=physionet2012_dataset['n_features'],\n",
        "    n_classes=physionet2012_dataset[\"n_classes\"],\n",
        "    n_layers=2,\n",
        "    d_model=physionet2012_dataset[\"n_features\"] * 4,\n",
        "    d_ffn=256,\n",
        "    n_heads=2,\n",
        "    dropout=0.3,\n",
        "    batch_size=32,\n",
        "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
        "    epochs=10,\n",
        "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
        "    # You can leave it to defualt as None to disable early stopping.\n",
        "    patience=3,\n",
        "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
        "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
        "    optimizer=Adam(lr=1e-3),\n",
        "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
        "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
        "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
        "    num_workers=0,\n",
        "    # just leave it to default as None, PyPOTS will automatically assign the best device for you.\n",
        "    # Set it as 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices, even parallelly on ['cuda:0', 'cuda:1']\n",
        "    device=None,\n",
        "    # set the path for saving tensorboard and trained model files\n",
        "    saving_path=\"tutorial_results/classification/raindrop\",\n",
        "    model_saving_strategy=\"best\", # only save the best model after training finished.\n",
        "                                  # You can also set it as \"better\" to save models performing better ever during training.\n",
        ")\n",
        "\n",
        "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
        "raindrop.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n",
        "\n",
        "# the testing stage\n",
        "raindrop_results = raindrop.predict(dataset_for_testing)\n",
        "raindrop_prediction = raindrop_results[\"classification\"]\n",
        "\n",
        "# calculate the values of binary classification metrics on the model's prediction\n",
        "metrics = calc_binary_classification_metrics(raindrop_prediction, dataset_for_testing[\"y\"])\n",
        "print(\"Testing classification metrics: \\n\"\n",
        "    f'ROC_AUC: {metrics[\"roc_auc\"]}, \\n'\n",
        "    f'PR_AUC: {metrics[\"pr_auc\"]},\\n'\n",
        "    f'F1: {metrics[\"f1\"]},\\n'\n",
        "    f'Precision: {metrics[\"precision\"]},\\n'\n",
        "    f'Recall: {metrics[\"recall\"]},\\n'\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}